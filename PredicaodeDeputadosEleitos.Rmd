---
title: "Predição de Deputados Eleitos 2014"
author: "Dandara Sousa"
date: "26 de fevereiro de 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(readr)
library(dplyr) 
library(ggplot2)


library(caret)
library(ROSE)
library(rpart)
library(rpart.plot) 

theme_set(theme_minimal())
```


```{r}
data <- read.csv("train.csv")
test <- read.csv("test.csv")
submission <- read.csv("sample_submission.csv")

#facilitar análises futuras
data$isDeputado = ifelse(data$descricao_ocupacao == "DEPUTADO",1,0)


#criar partição
dataPartition <- createDataPartition(y = data$situacao_final, p=0.75, list=FALSE);
data.train <- data[dataPartition, ]
data.test <- data[-dataPartition, ]
```

1. Há desbalanceamento das classes (isto é, uma classe tem muito mais instâncias que outra)? Em que proporção? Quais efeitos colaterais o desbalanceamento de classes pode causar no classificador?

Como visto abaixo, a classe marjoritária é a de não eleitos. Sendo assim, há um desbalanceamento onde eleitos são apenas cerca de 10% dos dados. Isso pode causar para o modelo uma desigualdade na hora de aprender a classificar o resultado final. Para isso, mais abaixo usaremos uma função para eliminar aleatoriamente linhas de deputados não eleitos igualando mais a situação.

```{r}
n_eleitos <- data %>% filter(situacao_final == "eleito") %>% nrow()
n_neleitos <- data %>% filter(situacao_final == "nao_eleito") %>% nrow()

df = data.frame(situacao=c("eleito","não eleito"),count = c(n_eleitos, n_neleitos))

ggplot(df,aes(x="", y=count , fill= situacao)) + 
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start=0)
  
```

```{r}
#fazer up pra balancear
formula = as.formula(situacao_final ~ partido + descricao_cor_raca + sexo + total_despesa + descricao_ocupacao + despesa_max_campanha)

fitControl <- trainControl(method = "repeatedcv",
                           search = "random"
                           )
model <- glm(formula = formula, data = data.train, family = "binomial")

summary(model)
```

```{r}

# os escolhidos são os que possuem *** ou ** dado o summary
data.train <- data.train %>%
  select(situacao_final, total_despesa, isDeputado, despesa_max_campanha)
data.test <- data.test %>%
  select(situacao_final, total_despesa, isDeputado, despesa_max_campanha)

#oversampling #undersampling

rose.train <- ROSE(situacao_final~., data = data.train)$data
table(rose.train$situacao_final)

formula = as.formula(situacao_final ~.)
model.escolhidos <- glm(formula = formula, data = rose.train, family = "binomial")
summary(model.escolhidos)
```

```{r}
#despesa_max sem importancia

rose.train <- rose.train %>%
  select(situacao_final,total_despesa,isDeputado)

control <- rpart.control(maxdepth = 20,
                         minsplit = 20,
                         cp = 0.01
                         )
arvore <- rpart(formula = formula, data = data.train, control = control)
prp(arvore)
```



```{r}
modelo2 <- train(formula = formula,
                 data = data.train,
                 method = "adaboost",
                 trControl = fitControl)

```


>Treine: um modelo de regressão logística, uma árvore de decisão e um modelo de adaboost. Tune esses modelos usando validação cruzada e controle overfitting se necessário, considerando as particularidades de cada modelo. 

>Reporte acurácia, precision, recall e f-measure no treino e validação. Como você avalia os resultados? Justifique sua resposta.

>Interprete as saídas dos modelos. Quais atributos parecem ser mais importantes de acordo com cada modelo? Crie pelo menos um novo atributo que não está nos dados originais e estude o impacto desse atributo

>Envie seus melhores modelos à competição do Kaggle. Sugestões abaixo:
   Experimente outros modelos (e.g. SVM, RandomForests e GradientBoosting)
   Crie novos atributos.
